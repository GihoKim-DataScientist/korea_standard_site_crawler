{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "collections.Callable = collections.abc.Callable\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "# json file로 변환하는 함수\n",
    "def transform_to_json(dic, file_name):\n",
    "    with open(file_name, 'w', encoding = 'UTF8') as f:\n",
    "        json.dump(dic, f, ensure_ascii=False, indent = '\\t')\n",
    "        \n",
    "        \n",
    "# text 데이터 정제 함수        \n",
    "def clean_data(string):\n",
    "    regexed_str = re.sub(\"[\\n]+\", \" \", string)\n",
    "    regexed_str = re.sub(\"[\\t]+\", \" \", string)\n",
    "    return_string = regexed_str.replace(\"\\n\", \" \")\n",
    "    return return_string.strip()\n",
    "\n",
    "\n",
    "# 테이블 형태 데이터 처리 함수\n",
    "def small_table_parser(contents_node, header_node):\n",
    "    total_lst = []\n",
    "    for contents_idx in range(0, len(contents_node), len(header_node.find_all(\"th\"))):\n",
    "        sub_dict = {}\n",
    "        for th in range(len(header_node.find_all(\"th\"))):\n",
    "            sub_dict[header_node.find_all(\"th\")[th].text] = clean_data(contents_node[contents_idx + th].text)\n",
    "        total_lst.append(sub_dict)\n",
    "    return total_lst\n",
    "\n",
    "\n",
    "# 표준 데이터 크롤러\n",
    "def std_crawler(whole_dict, lst_page_soup, driver, std_idx):\n",
    "    page_list_data = lst_page_soup.find_all(\"tbody\")[1]\n",
    "    row_lst = page_list_data.find_all(\"tr\")\n",
    "    \n",
    "    driver.implicitly_wait(100)\n",
    "    \n",
    "    for standard in range(std_idx, len(row_lst) + 1):\n",
    "        \n",
    "        # data 개수 체크\n",
    "        num = int(lst_page_soup.select(\"#tabs-container > div.table.list > table > tbody > tr:nth-of-type({}) > td:nth-of-type(1)\".format(standard))[0].text)\n",
    "        \n",
    "        driver.implicitly_wait(100)\n",
    "        \n",
    "        #crawling 작업\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"tabs-container\"]/div[2]/table/tbody/tr[{}]/td[2]/a'.format(standard)).click()\n",
    "            \n",
    "            std = {}\n",
    "            \n",
    "            result_html = driver.page_source\n",
    "            soup = BeautifulSoup(result_html, 'html.parser')\n",
    "            \n",
    "            # 테이블 row 개수\n",
    "            table_data = soup.find(\"div\", {'class':'table view'})\n",
    "            table_data = table_data.find('tbody').children\n",
    "\n",
    "            table_lst = []\n",
    "            for child in table_data:\n",
    "                table_lst.append(child)\n",
    "            for i in table_lst:\n",
    "                if i == \"\\n\":\n",
    "                    table_lst.remove(i)\n",
    "            \n",
    "                                    \n",
    "            for row in range(1, len(table_lst) + 1):\n",
    "                selector = \"#contents > div > div.content_inner > div.table.view > table > tbody > \"\n",
    "                title = soup.select(selector + \"tr:nth-of-type({}) > th\".format(row))[0].text\n",
    "                \n",
    "                # 인용표준, 기술기준 데이터 / index text를 비교하는 if 문으로 바꾸기 / 가독성 고려\n",
    "                if title == \"인용표준\" or title == \"기술기준\":\n",
    "                    title = soup.select(selector + \"tr:nth-of-type({}) > th\".format(row))[0].text\n",
    "                    content = soup.select(selector + \"tr:nth-of-type({}) > td\".format(row))[0].text\n",
    "                    \n",
    "                    title_2 = soup.select(selector + \"tr:nth-of-type({}) > th:nth-of-type(2)\".format(row))[0].text\n",
    "                    content_2 = soup.select(selector + \"tr:nth-of-type({}) > td:nth-of-type(2)\".format(row))[0].text\n",
    "                    \n",
    "                    \n",
    "                    content = clean_data(content)\n",
    "                    content_2 = clean_data(content_2)\n",
    "                    \n",
    "                    std[title] = content\n",
    "                    std[title_2] = content_2\n",
    "                    \n",
    "                #테이블 데이터 처리 / 유형별로 파악해서 if 문 바꾸기 / 아예 클래스를 만들어서 기존 항목에 추가 제거의 경우에 처리할 경우 / 테이블 파싱하는 함수 만들기 (colspan 예외 처리도)\n",
    "                elif title == \"국제표준 부합화\" or title == \"표준이력사항\" or title == \"인증심사기준\":\n",
    "                    std[title] = []\n",
    "                    \n",
    "                    # 국제표준 부합화\n",
    "                    if title == \"국제표준 부합화\":\n",
    "                        sub_dict = {}\n",
    "                        data = soup.find(\"div\", {'class':\"table list gray\"})\n",
    "                        contents = data.find_all(\"td\")\n",
    "                        \n",
    "                        if \"colspan\" in contents[0].attrs:\n",
    "                            for j in range(int(contents[0].attrs[\"colspan\"])):\n",
    "                                sub_dict[data.find_all(\"th\")[j].text] = contents[0].text\n",
    "                            std[title].append(sub_dict)\n",
    "                            contents = contents[1:]\n",
    "                            \n",
    "                        table_data_lst = small_table_parser(contents, data)\n",
    "                        for tb_data in table_data_lst:\n",
    "                            std[title].append(tb_data)\n",
    "                        \n",
    "                    #표준이력사항\n",
    "                    elif title == \"표준이력사항\":\n",
    "                        data = soup.find_all(\"div\", {'class':\"table list gray\"})[1]\n",
    "                        contents = data.find_all(\"td\")\n",
    "                        \n",
    "                        table_data_lst = small_table_parser(contents, data)\n",
    "                        std[title] = table_data_lst\n",
    "                            \n",
    "                    #인증심사기준\n",
    "                    else:\n",
    "                        sub_dict = {}\n",
    "                        data = soup.find_all(\"div\", {'class':\"table list gray\"})[2]\n",
    "                        contents = data.find_all(\"td\")\n",
    "                        if len(contents) == 1:\n",
    "                            for i in range(int(contents[0].attrs[\"colspan\"])):\n",
    "                                sub_dict[data.find_all(\"th\")[i].text] = contents[0].text\n",
    "                            std[title].append(sub_dict)\n",
    "                        \n",
    "                        else:\n",
    "                            table_data_lst = small_table_parser(contents, data)\n",
    "                            std[title] = table_data_lst\n",
    "\n",
    "                # 나머지 데이터\n",
    "                else:\n",
    "                    content = soup.select(selector + \"tr:nth-of-type({}) > td\".format(row))[0].text\n",
    "                    content = clean_data(content)\n",
    "                    std[title] = content\n",
    "                    \n",
    "            now = time.localtime()\n",
    "            now = \"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "            std['crawled_time'] = now\n",
    "            whole_dict['stds'].append(std)\n",
    "            \n",
    "            driver.back()\n",
    "            driver.refresh()\n",
    "            \n",
    "        except:\n",
    "            print(\"Crawling error, No : \", num)\n",
    "            is_success = False\n",
    "            return whole_dict, is_success, standard\n",
    "    \n",
    "        driver.implicitly_wait(100)\n",
    "    \n",
    "    std_idx = 1    \n",
    "    is_success = True\n",
    "    return whole_dict, is_success, std_idx\n",
    "\n",
    "\n",
    "# 페이지 넘기는 크롤러\n",
    "def page_crawler(start_page, chrome_path, url, std_idx):\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path)\n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.find_element(By.CLASS_NAME, \"last\").click()\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    last_page = soup.find_all(\"a\", {\"class\": \"on\"})[1].text\n",
    "    driver.find_element(By.CLASS_NAME, \"first\").click()\n",
    "    \n",
    "    std_dict = {}\n",
    "    std_dict['stds'] = []\n",
    "    \n",
    "    start_page = int(start_page)\n",
    "    if start_page % 10 == 0:\n",
    "        num_next = start_page // 10 - 1\n",
    "    else:\n",
    "        num_next = start_page // 10\n",
    "\n",
    "    for i in range(num_next):\n",
    "        driver.find_element(By.CLASS_NAME, 'next').click()\n",
    "        \n",
    "    page_source = driver.page_source\n",
    "    lst_page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 페이지 버튼 갯수\n",
    "    page_button_data = lst_page_soup.find(\"div\", {'class':\"page\"})\n",
    "    li_lst = page_button_data.find_all(\"li\")\n",
    "    li_lst = [button.text for button in li_lst]\n",
    "    start = li_lst.index(str(start_page))\n",
    "    \n",
    "    is_success = False\n",
    "    page_num = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            for page in range(start + 1, len(li_lst) - 1):\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"tabs-container\"]/div[3]/div/div/ul/li[{}]/a'.format(page)).click()\n",
    "                page_num = driver.find_element(By.XPATH, '//*[@id=\"tabs-container\"]/div[3]/div/div/ul/li[{}]/a'.format(page)).text\n",
    "                page_source = driver.page_source\n",
    "                lst_page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                driver.implicitly_wait(100)\n",
    "                \n",
    "                std_dict, is_success, std_idx = std_crawler(std_dict, lst_page_soup, driver, std_idx)\n",
    "                \n",
    "                if page_num == last_page:\n",
    "                    is_success = True\n",
    "                    return std_dict, int(page_num), is_success, std_idx\n",
    "                \n",
    "                elif is_success == False:\n",
    "                    return std_dict, int(page_num), is_success, std_idx\n",
    "                    \n",
    "                \n",
    "            # 다시 1페이지부터\n",
    "            start = 2\n",
    "            \n",
    "            driver.find_element(By.CLASS_NAME, 'next').click()\n",
    "            \n",
    "    except:\n",
    "        return std_dict, int(page_num), is_success, std_idx\n",
    "    \n",
    "\n",
    "# json 파일 dataframe으로 읽는 함수\n",
    "def json_reader(filename):\n",
    "    with open (filename, \"r\", encoding = 'UTF8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.json_normalize(data['stds'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# DB에 테이블이 생성되어 있다면 테이블에 데이터 넣는 함수\n",
    "def db_process(filename, host, password, schema_name, table_name):\n",
    "    try:\n",
    "        db = pymysql.connect(host = host, port = 3306, user = 'root', password = password,\n",
    "                        db = schema_name, charset = 'utf8')   # charset: 인코딩 설정\n",
    "    except: \n",
    "        print(\"Can not connent to DB server. Please check the connection\")\n",
    "        return\n",
    "    \n",
    "    cursor = db.cursor()\n",
    "\n",
    "    sql = 'INSERT INTO ' + table_name + ' (doc_num, doc_name_ko, doc_name_en, publish_date, final_date, json_data, crawled_time, tag) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)'\n",
    "\n",
    "    output_df = json_reader(filename)\n",
    "    output_df = output_df.reset_index()\n",
    "\n",
    "    with open (filename, \"r\", encoding = 'UTF8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    lst = data['stds']\n",
    "            \n",
    "    for i in range(len(output_df)):\n",
    "        json_data = json.dumps(lst[i])\n",
    "        try:\n",
    "            cursor.execute(sql, (output_df.loc[i][\"표준번호\"], output_df.loc[i][\"표준명(한글)\"], output_df.loc[i][\"표준명(영문)\"], output_df.loc[i][\"제정일\"], output_df.loc[i][\"최종개정확인일\"], json_data, output_df.loc[i][\"crawled_time\"], filename[7:15]))\n",
    "        except:\n",
    "            if len(output_df.loc[i][\"제정일\"]) == 0:\n",
    "                cursor.execute(sql, (output_df.loc[i][\"표준번호\"], output_df.loc[i][\"표준명(한글)\"], output_df.loc[i][\"표준명(영문)\"], None, output_df.loc[i][\"최종개정확인일\"], json_data, output_df.loc[i][\"crawled_time\"], filename[7:15]))\n",
    "            else:\n",
    "                print(\"There is a problem while inserting data number : \", i)\n",
    "    db.commit()\n",
    "\n",
    "    db.close()\n",
    "    print(\"Done with inserting data to DB\")\n",
    "    return\n",
    "\n",
    "\n",
    "# main 함수 = crawling --> json data --> DB 연동 및 데이터 inserting\n",
    "def main(last_page_no, max_retry_num, chrome_path, url, host, password, schema_name, table_name):\n",
    "    data_list = {}\n",
    "    data_list['stds'] = []\n",
    "    current_retry_count = 0\n",
    "    std_idx = 1\n",
    "    now = datetime.datetime.now()\n",
    "    output_name = \"output_{}.json\".format(now.strftime('%Y%m%d'))\n",
    "    \n",
    "    while True:\n",
    "        if current_retry_count >= max_retry_num:\n",
    "            print(\"number of retry exceeds maximum retry number\")\n",
    "            print(\"last page of crawling is \", last_page_no)\n",
    "            transform_to_json(data_list, output_name)\n",
    "            break\n",
    "            # return data_list, is_success, current_retry_count\n",
    "        \n",
    "        current_data_list, end_page_no, is_success, std_idx = page_crawler(last_page_no, chrome_path, url, std_idx)\n",
    "        \n",
    "        # merge current_data_list to data_list \n",
    "        for i in current_data_list['stds']:\n",
    "            if i not in data_list['stds']:\n",
    "                data_list['stds'].append(i)\n",
    "                \n",
    "        # if it couldn't reach the last page\n",
    "        if is_success == False:\n",
    "            current_retry_count = current_retry_count + 1\n",
    "            transform_to_json(data_list, output_name)\n",
    "            last_page_no = end_page_no\n",
    "\n",
    "            driver_2 = webdriver.Chrome(executable_path=chrome_path)\n",
    "            driver_2.get(url)\n",
    "\n",
    "            for i in range(end_page_no // 10):\n",
    "                driver_2.find_element(By.CLASS_NAME, 'next').click()\n",
    "                \n",
    "            page_source = driver_2.page_source\n",
    "            lst_page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            sub_dict = {}\n",
    "            sub_dict['stds'] = []\n",
    "\n",
    "            not_success_dict, is_sucess, standard = std_crawler(sub_dict, lst_page_soup, driver_2, std_idx)\n",
    "            \n",
    "            for i in not_success_dict['stds']:\n",
    "                current_data_list['stds'].append(i)\n",
    "\n",
    "            print(\"Crawling is finished with the error page : \", last_page_no)\n",
    "\n",
    "            last_page_no = last_page_no + 1\n",
    "\n",
    "            print(\"Reconnecting to the page number : \", last_page_no)\n",
    "        \n",
    "        # if it reaches the last page\n",
    "        elif is_success:\n",
    "            print(\"Done with crawling\")\n",
    "            transform_to_json(data_list, output_name)\n",
    "            break\n",
    "            # return data_list, is_success, current_retry_count\n",
    "            \n",
    "    return db_process(output_name, host, password, schema_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e51ca8c55b2f>:163: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling error, No :  8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e51ca8c55b2f>:300: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver_2 = webdriver.Chrome(executable_path=chrome_path)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://standard.go.kr/KSCI/standardIntro/getStandardSearchList.do?menuId=919&topMenuId=502\"\n",
    "main(890, 10, r\"C:\\Users\\gihok\\chatbot\\chromedriver.exe\", url, \"192.168.0.124\", \"linuxer\", \"std_crawled_data\", \"std_data_check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-9267a49a95a8>:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=r\"C:\\Users\\gihok\\chatbot\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=r\"C:\\Users\\gihok\\chatbot\\chromedriver.exe\")\n",
    "driver.get(\"https://standard.go.kr/KSCI/standardIntro/getStandardSearchList.do?menuId=919&topMenuId=502\")\n",
    "\n",
    "for i in range(89):\n",
    "    driver.find_element(By.CLASS_NAME, 'next').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = driver.page_source\n",
    "lst_page_soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '//*[@id=\"tabs-container\"]/div[2]/table/tbody/tr[{}]/td[3]/a'.format(1)).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dict = {}\n",
    "whole_dict['stds'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict, is_sucess, standard = std_crawler(whole_dict, lst_page_soup, driver, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'표준번호': 'KS C IEC60601-2-8', '표준명(한글)': '의료용 전기기기 ─ 제2-8부: 10 kV에서 1 MV 범위의 치료용엑스선조사장치의 기본 안전 및 필수 성능에 관한 개별 요구사항', '표준명(영문)': 'Medical electrical equipment ─ Part 2-8: Particular requirements for the basic safety and essential performance of therapeutic X-ray equipment operating in the range 10 kV to 1 MV', 'KS원문보기': '표준 원문보기', '표준분야': '(C) 전기전자 - (C22) 기타', '표준구분': '제품', '제정일': '2002-10-08', '최종개정확인일': '2021-06-28', '기술심의회': '의료용전기제품 기술심의회(A,C,P,X)', '전문위원회': 'IEC/TC 62/SC C(방사선치료,핵의학 및 ~)', '적용범위': '이 표준은 교류 주전원에 연결되었을 때 공칭 X선관 전압이 10 kV ~ 1 MV 범위에 있는 치료용 X선 장치(이하 \"ME 기기\"라 한다)의 기본 안전 및 필수 성능에 적용한다.', '표준개발협력기관': '한국원자력의학원', '담당부처': '식품의약품안전처', '담당부서': '의료기기정책과', '담당자명': '정상오', '담당자 연락처': '043-719-5660', '국제표준 부합화': [{'대응국제표준': 'IEC 60601-2-8 Ed. 2.1', '부합화수준': 'IDT 일치'}], '인용표준': '4 건', '기술기준': '0 건', 'ICS Code': '11.040.50 (방사선 및 기타 진단 기기)', '표준이력사항': [{'변경일자': '2002-10-08', '구분': '제정', '고시번호': '2002-1234', '제정,개정,폐지 사유': '', '신구대비표': ''}, {'변경일자': '2007-08-31', '구분': '확인', '고시번호': '2007-0572', '제정,개정,폐지 사유': '5년 도래규격으로 별다른 개정사항이 없으므로 확인함.', '신구대비표': ''}, {'변경일자': '2012-12-20', '구분': '개정', '고시번호': '2012-0726', '제정,개정,폐지 사유': '치과용 의료기기분야 국가표준을 정비하여 관련 산업의 경쟁력을 높이고 안전관리 및 수출입 의료기기제품의 품질관리에 활용하기 위함', '신구대비표': ''}, {'변경일자': '2017-12-29', '구분': '개정', '고시번호': '2017-106', '제정,개정,폐지 사유': '국제표준부합화에 의한 개정', '신구대비표': ''}, {'변경일자': '2021-06-28', '구분': '개정', '고시번호': '제2021-49호', '제정,개정,폐지 사유': '자체 검토에 의한 개정', '신구대비표': ''}], '인증심사기준': [{'제/개정일자': '관련 자료가 없습니다.', '구분': '관련 자료가 없습니다.', '첨부파일': '관련 자료가 없습니다.'}], 'crawled_time': '2022/08/29 00:53:45'}\n"
     ]
    }
   ],
   "source": [
    "for i in sample_dict['stds']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1941832dfff7dd015a41b2ce945c20a46564c19e0b8f92c654b394a1da7b9dfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
